import base64
import hashlib
import os
import pickle
import time
from threading import Lock

from dotenv import load_dotenv
from google import genai

# Define the path to the cache file
CACHE_FILE = "./llm_cache.pkl"

load_dotenv()

_llm_cache_lock = Lock()
_llm_cache = {}

# Load the existing cache or initialize a new dictionary
if os.path.exists(CACHE_FILE):
    with open(CACHE_FILE, "rb") as f:
        _llm_cache = pickle.load(f)

prices = {
    "gemini-2.5-pro": {
        "input_price_per_million": 1.25,
        "output_price_per_million": 10,
    },
    "gemini-2.5-flash": {
        "input_price_per_million": 0.3,
        "output_price_per_million": 2.5,
    },
    "gemini-2.5-flash-lite": {
        "input_price_per_million": 0.1,
        "output_price_per_million": 0.4,
    },
    "gemini-2.0-flash": {
        "input_price_per_million": 0.1,
        "output_price_per_million": 0.4,
    },
    "gemini-2.0-flash-lite": {
        "input_price_per_million": 0.075,
        "output_price_per_million": 0.3,
    },
}


def text2text(prompt, model_name="gemini-2.5-flash", cache=True):
    """
    Calls a language model (LLM) API to generate a response based on the given prompt.

    This function uses caching to avoid redundant API calls for the same prompt and model.
    If a cached response exists, it is returned directly. Otherwise, the function interacts
    with the OpenAI API to generate a response, handles potential errors with retries, and
    stores the result in the cache.

    Args:
        prompt (str): The input text or query to send to the language model.
        model (str, optional): The name of the language model to use. Defaults to "gpt-4o".

    Returns:
        str: The response generated by the language model.
        float: The cost of the API call in USD, if applicable (e.g., for Gemini models).

    Raises:
        Exception: If an error occurs during the API call, it retries indefinitely with a delay.

    Notes:
        - The function requires an environment variable `OPENAI_API_KEY` to authenticate with the API.
        - Responses are cached using a SHA-256 hash of the prompt and model name as the key.
        - The cache is persisted to a file defined by the `CACHE_FILE` variable.
    """
    global _llm_cache
    global _llm_cache_lock

    if cache:
        # Generate a unique key based on the prompt and model
        cache_key = hashlib.sha256(f"{model_name}:{prompt}".encode("utf-8")).hexdigest()
        # Check if the response is already in the cache
        with _llm_cache_lock:
            if cache_key in _llm_cache:
                return _llm_cache[cache_key], 0.0

    if "gemini" in model_name:
        client = genai.Client(
            vertexai=True, project=os.getenv("PROJECT_ID"), location="us-east1"
        )

        while True:
            try:
                response = client.models.generate_content(
                    model=model_name, contents=prompt
                )
                break
            except Exception as e:
                print(f"⚠️ LLM API error: {e}, retrying in 10 seconds...")
                time.sleep(10)

        input_token = response.usage_metadata.prompt_token_count
        output_token = response.usage_metadata.candidates_token_count
        price = input_token * prices[model_name]["input_price_per_million"]
        price += output_token * prices[model_name]["output_price_per_million"]
        price /= 1000000
        response = response.text

    # Save the response in the cache
    if cache:
        with _llm_cache_lock:
            _llm_cache[cache_key] = response

    return response, price


def _hash_file(path: str) -> str:
    """Returns the SHA-256 of the file (read in chunks)."""
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(1024 * 1024), b""):
            h.update(chunk)
    return h.hexdigest()


def wavtext2text(
    wav_path: str,
    prompt: str | None = None,
    model_name: str = "gemini-2.5-flash",
    cache: bool = True,
):
    """
    Transcribes/Understands a WAV audio file via a Gemini multimodal model.

    Args:
        wav_path: path to the .wav file
        prompt: (optional) instruction/guidance to steer the transcription or summary
        model_name: Gemini model (must support audio, e.g. 'gemini-2.5-flash' or 'gemini-2.0-flash')
        cache: enables cache based on (model_name, file_hash, prompt)

    Returns:
        (response_text, price_usd)
    """
    global _llm_cache
    global _llm_cache_lock

    if not os.path.exists(wav_path):
        raise FileNotFoundError(f"File not found: {wav_path}")

    # Cache key: model + file hash + prompt
    file_hash = _hash_file(wav_path)
    prompt_key = prompt or ""
    cache_key = hashlib.sha256(
        f"{model_name}:{file_hash}:{prompt_key}".encode("utf-8")
    ).hexdigest()

    if cache:
        with _llm_cache_lock:
            if cache_key in _llm_cache:
                return _llm_cache[cache_key], 0.0

    # Prepare audio content (inline, base64)
    with open(wav_path, "rb") as f:
        audio_b64 = base64.b64encode(f.read()).decode("utf-8")

    # Build the multimodal message
    contents = [
        {
            "role": "user",
            "parts": [
                {
                    "inline_data": {
                        "mime_type": "audio/wav",
                        "data": audio_b64,
                    }
                },
            ],
        }
    ]
    if prompt:
        contents[0]["parts"].append({"text": prompt})

    # Gemini API call via Vertex AI (same retry logic)
    client = genai.Client(
        vertexai=True, project=os.getenv("PROJECT_ID"), location="us-east1"
    )

    while True:
        try:
            response = client.models.generate_content(
                model=model_name, contents=contents
            )
            break
        except Exception as e:
            print(f"⚠️ LLM API error: {e}, retrying in 10 seconds...")
            time.sleep(10)

    # Cost (same method as text2text)
    input_token = getattr(response.usage_metadata, "prompt_token_count", 0)
    output_token = getattr(response.usage_metadata, "candidates_token_count", 0)
    price = 0.0
    if model_name in prices:
        price = (
            input_token * prices[model_name]["input_price_per_million"]
            + output_token * prices[model_name]["output_price_per_million"]
        ) / 1_000_000

    response_text = getattr(response, "text", "")

    if cache:
        with _llm_cache_lock:
            _llm_cache[cache_key] = response_text

    return response_text, price


def save_cache():
    global _llm_cache
    global _llm_cache_lock
    with _llm_cache_lock:
        with open(CACHE_FILE, "wb") as f:
            pickle.dump(_llm_cache, f, protocol=pickle.HIGHEST_PROTOCOL)
